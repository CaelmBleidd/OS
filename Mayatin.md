# ОСИ
## Курс Маятина 
Курс не нацелен на написание системного ПО самостоятельно, вместо этого будем разрабатывать ПО, которое будет работать под управлением системы. 

Сначала чуть больше половины -- классический курс ОС. Поговорим про процессы, ядра, архитектуру, процессы, планирование, проблемы синхронизации процессов,  тупики и проблемы, связанные с тупиками, выделение памяти, файловые системы. 

Вторая часть -- более современные вещи: проблемы синхронизации на различных узлах, распределённые системы, облака, безопасность. 


## Первая лекция
Программные диспетчеры появились в конце 40-ых, начале 50-ых годов. Сначала код каждый раз загружался заново в память, программа выполнялась, код выгружался. 
Позже появились идеи выделить некоторую часть RAM, чтобы загрузить туда часть часто используемых инструкций (микропрограмм). Появилась простейшая линковка. Дальше оказалось, что не очень удобно, когда это хранится в статических адресах, но на тот момент всё размещалось статически. 
Когда выяснилось, что RAM всегда не хватает, решили использовать spooling, чтобы загружать данные в RAM параллельно. Появился контроллер, контролирующий управление данными. Появился механизм interrupt. 

Позже появилось понятие "пакет" -- совокупность программных компонентов и, возможно, данных, связанных с ними. При этом становится логичным, чтобы storage хранила последовательность пакетов, которые потом будут выполнены. Появляется очередь. Поэтому начинают появляться первые планировщики. 


[Фоточка-картиночка архитектуры на втором этапе](https://lh3.googleusercontent.com/OhKCqwiDnW6cbS2WBOYTnQJfq3RGMOIAD1mek4Y5zgw-3RkT8f-6HOhMvXeoIIzUOoAKC2krJWP-)

### Второй этап
Этап многозадачных операционных систем. Процесс выполнения начинает разбиваться на две явные фазы: обработка данных и операции ввода-вывода. 
Появляется концепция разделения времени. Сложнее оказалось разделить память. После появления этой проблемы появилась концепция виртуальной памяти. Появляется механизм защиты памяти и появления привилегированного режима. Концепция виртуальной машины для программ.

В 1963 году появляется MCP. Её в описании назвали уже "операционной системой". Она поддерживала всё, описанное выше, мультипроцессорную обработку. 

### Третий этап
Сетевые операционные системы. 
В те году AT&T соединил огромное количество городов сетью. 
Появляется понятие "Терминал". 
Идентификация -> аутентификация -> авторизация. Данная концепция появляется, чтобы разграничить права доступа. 
Однако возникает проблема простоя некоторых узлов. Появляется понятие компьютерной сети. В конце 60-ых завершается третий этап. 

### Четвёртый этап
Появление "мобильных ОС". До этого момента операционная система создавалась **уникально** для каждого компьютера. На этом этапе появилось абстрагирование ПО от конкретных компьютеров. 
Появление UNICS. В 1967 году запускается проект -- multics (?). Кенн Томпсон решает, что неплохо было бы перенести принципы этой ОС на все компьютеры. Для этого нужен был язык высокого уровня. 
Сначала пишут первую версию к первому января 1970 года целиком на ассемблере. 
Создаётся язык B, язык высокого уровня. На нем создаётся компилятор и начинает переписываться ядро ОС. Это происходит где-то к 1972 году, second edition. 
Разрабатывается язык С. Для него сначала пишут компилятор, затем прямо на С пишут ядро ОС. В 1975 году появляется полностью на C edition five и имеющая встроенный компилятор С. UNICS становится UNIX
В последней седьмой версии появляется предшественник Bash'a.
Из-за законодательства США появляется ситуация, когда AT&T не могут защитить патентами код UNIX. Пользуясь теми моментами, когда код ещё не был защищён, университеты США (в частности, Беркли) забирает себе исходный код UNIX и начинает развивать собственную ветку. Создаётся собственная дочерняя компания BSD. Противник Беркли, Стэнфорд, создаёт SUN, которое создаёт SUN OS, на котором работает ЦДО ИТМО. В это время AT&T удаётся закрыть код UNIX патентами. 

В восьмидесятые годы наступает расцвет ОС. Появление проекта Darwin, из которого позже разовьётся mac OS. 
Появляется идея, связанная со свободой. Все начинают понимать, что развитие ОС тормозится патентами. Ричард Столлман, преподаватель MIT, начинает продвигать идею создания ОС, которую невозможно было бы закрыть патентами. Мы делаем код, публикуем его под такой лицензией, чтобы если ты взял наш код и хотя бы как-то дополнил его, ты обязан выпустить свой продукт под этой же лицензией. 
Разработчики, включая Столлмана, переписывают компилятор С полностью с нуля, чтобы создать полностью свободный компилятор. Надо решить непростую задачу -- написать ядро. Танненбаум стартует свой проект minix, выступает с докладом про него. Во время доклада возникает спор между молодым аспирантом из аудитории и Танненбаумом. Этим аспирантом был ~~Эйнштейн~~ Линус Торвальдс. 
Создаётся GNU is not unix. Появление GNU/Linux. 

## Вторая лекция
/* куча нудных слов непонятно о чём (ни о чём)  */
Задача оптимизации процессов, в широком смысле, не может быть решена, поскольку мы не можем оценить оптимальность потраченного времени из-за затраты ресурсов системы на мониторинг, куда было потрачено время.
/* куча нудных слов и история про аэропорт */

Задачи ОС: 
(5 пунктов, которые я пропустил)

Функции ОС:
(1 пункт, который я тоже пропустил)

У меня есть некоторое количество мощностей и критериев K, которым я хочу удовлетворять. Введём __суперкритерий__ K с крышечкой, который будет некоторой свёрткой всех критериев с некоторыми коэффициентами. 

Для систем реального времени используют __условный критерий__: я пытаюсь найти максимум от некоторой свёрткой, при условии, что некоторый конкретный критерий находится в некоторых границах. 

Для того, чтобы найти эти коэффициенты при критериях, используют цикл __PDCA__ (Plan--Do--Check--Act).

ОС проектируются десятилетиями и на десятилетия, поэтому практически отсутствует возможность прогнозирования, в каких условиях она будет работать. 

### Уровень управления процессами
С точки зрения ОС процесс -- это структура данных. 

На этом слова кончились, дальше обещано что-то адекватное

## Третья лекция
### Архитектура операционной системой
Принципы:
1. Модульная организация
2. Функциональная избыточность 
3. Функциональная избирательность
4. Выделение хотя бы двух пространств: __user space__ и __kernel space__

**Привилегия** -- возможность доступа к физическим адресам памяти, а не приоритет в каких-либо операциях. Только ядро в привилегированном режиме имеет доступ к физическим устройствам и памяти. 
Кроме того, что ядро должно быть привилегированно, оно ещё должно быть __резидентно__, т.е. полностью и всегда расположено в оперативной памяти. 

Дальше возник спор: а что тогда должно быть в ядре? 
N.B. Ядро linux на момент написания данного конспекта включает приблизительно 8млн строк кода. 

Самый первый вид, о котором мы говорим -- __монолитное ядро__. Ядро особо не структурировано в плане доступа. Любая процедура может вызвать и передать данные любой другой процедуре. Тем не менее, даже в монолитной структуре выделяют три слоя, которые абстрагированы друг от друга:
1. Main program 
2. Services procedures 
3. Utilities

__Системный вызов__ -- это обращение пользовательской программы к ядру ОС с требованием предоставить дополнительные ресурсы или выполнить привилегированную операцию.
Как это устроено: у каждого приложения, которое запускается, выделяется некоторая область памяти, в которой находятся некоторые параметры. 
Дальше, если необходим системный вызов, программа заполняет это пространство и делает программное прерывание. Система это замечает, смотрит на номер системного вызова, делает валидацию параметров, после чего main program определяет, какой сервис будет отвечать за выполнение системного вызова. 

Утилиты, по большому счёту, это драйверы. Утилиты -- это то, что взаимодействует непосредственно с железом. 

Какие минусы быстро возникли:
* если я хочу что-то поменять в работе ОС, то, как минимум, это перезапуск ядра, как максимум -- перекомпиляция ядра
* что-то ещё, но я провафлил


Плюсы:
* никаких накладных расходов -> быстродействие 
* максимальная надёжность
/* стори про сервак факультета и FREE BSD */

### Многослойное ядро
Его иногда считают подмножеством монолитных ядер. Действительно, оно остаётся монолитным, просто количество слоёв увеличивается и появляется некоторое количество абстракций, которое позволяет переписывать отдельные уровни без влияния на остальные. 

Уровень аппаратной поддержки ядра:
* работа с прерываниями
* работа с контекстом 

Следующий уровень: **Hardware Abstraction Layer (HAL)**
Вы должны сделать абстрагирования всякого кода, связанного с принятием решений и тд, от платформы. Взаимодействие двух этих уровней позволяет работать системам на разных платформах. То, то раньше было утилитами, во многом переехало на HAL уровень. 

За HAL'ом идёт следующий уровень, который обычно называют **базовые механизмы ядра**. Будем рассматривать его в совокупности со слоем **менеджеров ресурсов**. 
Мы решили, что мы выделяем данному процессору некоторое адресное пространство. Теперь нужно его выделить. Эти процессы лучше разбить на разные уровни, потому что мы хотим, чтобы мы могли менять менеджер ресурсов (алгоритмы, подходы и тд), а исполнительский уровень не хотелось бы трогать. 

Последний слой: **слой системных вызовов**
Это тот слой, который нам обеспечивает взаимодействие находящегося снаружи программного обеспечения с нашим ядром. Сейчас этот слой предоставляет не одну точку входа (main program), а среды окружения. 

Где мы победили:
* можем переписывать отдельные слои

Где мы **не** победили:
* по прежнему съедаем большое количество памяти
* без перезапуска всё ещё сложно заменить что-то в ядре

### Новые концепции
Выполнение части задач на других аппаратных узлах. В чём проблемы? Менеджер ресурсов один. Он привязан к железу и не может ничего делать с другими узлами. 
Как выйти из этой ситуации? Нужно уходить из ядра, однако это сломает безопасность. Кроме того, это вызовет накладные расходы. 
Таким образом появилась **микроядерная архитектура**.
##

У меня есть user mode и kernel mode. 

В kernel mode я оставлю:
* базовые механизмы
*  HAL
* аппаратную поддержку

 В user mode разместим сервера. 
 
 [Схемка многоядерной архитектуры](https://lh3.googleusercontent.com/bjD2EEE3S5gjq--XqWFnS_MrAJ9cD_hy4UiP43rBgGrXAzw6jmEr7XQ-aryatQ6GnZN6F2n3p1km "Схема микроядерной архитектуры &#40;справа затесалась многослойная&#41;")

Проблема: мы часто не используем сервера, но они продолжают висеть в системе и жрать ресурсы. Что делать?
Не использую сервер печати? В файл подкачки его. Не использую сервер сети? В файл подкачки. Разница в 60-70 раз по ресурсам. 
Следующее преимущество подобного подхода: возможность создания распределённых систем. 

Чем за это заплатили? 
* производительностью 
* чем-то ещё

### Наноядро
По факту, в ядре остаётся только обработка прерываний. Всё остальное вынесено за пределы ядра, а ядро просто обрабатывает прерывание. 
Где и зачем это применяется: так работает виртуализация (домашняя). При серверной виртуализации используются другие подходы, потому что иначе слишком высокие накладные расходы. 

### Экзоядерная архитектура (инвертированная)
В ней мы в ядре оставляем планировщики и только их. Тогда мы теряем жёсткость контроля над оборудованием. Что мы приобретаем? Возможность работы при нестабильности железа. Любой сбой любого драйвера? Ничего страшного. Правда мы не гарантируем никакой консистентности и сохранности данных. 

##
Все реальные ОС имеют **гибридные ядра**, поэтому большинство из них называют **гибридными**. 
**windows** -- в чистом виде гибридная архитектура
**linux** -- монолитное ядро, но с оговорками (многопоточное ядро, отдельные программные компоненты, в том числе на HAL уровне, могут быть заменены без перезагрузки при помощи заглушек)
**Mac OS** -- микроядро

## Четвёртая лекция
### Процесс
Попытаемся разобраться, что такое процесс и что такое поток. 

**Процесс** -- совокупность набора исполняющихся команд, ассоциированных с ним ресурсов и текущего статуса их выполнения, находящаяся под управлением ОС. 

Нельзя ставить чёткое соответствие между программами и процессами. Например, одна программа может создавать больше одного процесса. В то же время, один процесс может быть совокупностью нескольких программ. 

Что значит ".... и ассоциированных с ним ресурсов". Если я запущу один и тот же файл несколько раз, набор исполняемых команд будет тот же самый, но ведь это другой процесс. Значит, он будет отличаться ресурсами, хотя бы адресными пространствами и файловыми дескрипторами. В UNIX любой процесс на старте получит три дескриптора: `stdin`, `stdout`, `stderr` с номерами 0, 1 и 2 соответственно. Ещё ассоциируются обычно сетевые порты (сокеты) и тд. 

"... и текущего статуса исполнения". С точки зрения существования ОС каждый процесс характеризуется его состоянием. Даже на уровне интуиции понятно, что процесс может либо исполняться, либо находиться в ожидании какого-либо процесса, либо находится в зомби-состоянии и тд. Этот процесс характеризуется этим состоянием, которое характеризуется его контекстом (состоянием регистров в последний момент исполнения). 

Наконец, последняя часть определения. ОС имеет монополию над процессами, соответственно, только она имеет полную власть над ними. 

20 лет назад стало очевидно, что многие задачи с большими массивами данных прекрасно распараллеливаются. Возникает вопрос, как разделить этот массив эффективно в условиях, когда мы не знаем, какие ресурсы будут нам доступны. Хочется, чтобы процессы по одному выбирали просто какой-то следующий доступный блок данных, когда потребуется. Тогда мы действительно получим преимущество по скорости. 

Появляется противоречие с концепцией изоляции процессов друг от друга. Для того, чтобы это исправить, сделали вполне понятный ход: давайте разрешим внутри одного процесса существовать нескольким потокам исполняющихся команд. 
Тогда мы переходим от понятия "процесса" к понятию **потока**. Сегодня любая ОС работает следующим образом: немного изменилось определение процесса. Сегодня процесс -- это контейнер, в котором существует как минимум один поток исполнения, в котором существует уже поток исполнения команд. 
**Процесс** -- контейнер ресурсов для совокупности потоков. Теперь любая ОС осуществляет планирование на уровне последних. 

``` mermaid
graph LR
Z[job] --> A
A[Process] --> B[Thread1] 
A[process] --> C[Thread2]
A[process] --> D[ThreadN]
B --> E[Fiber1]
B --> F[FiberM]
```
Система имеет монополию управления потоков. В ответ на запрос пользователей создаются отдельные волокна (fiber), которые позволяют управлять планировкой времени на уровне волокон. Линукс отказался от этой идеи с самого начала, MS же поддерживает. 
Дальше началась развиваться концепция, которая была хорошо забытым старым, потому что даже трёх уровней стало не хватать. Появился **job** (group). 
Например, есть chrome. У него много тяжелых вкладок, на которых много тяжёлого кода. Хотелось бы, чтобы вкладки выполнялись в разных процессах, чтобы краш одной вкладки не приводил к крашу браузера. Так же, если бы браузер использовал только один процесс, можно было бы спереть данные из одной вкладки через другую. Если у меня открыто 100 вкладок хрома и одна вкладка ворда, ворд получит 1/101 процессорного времени. Несправедливо. Так появился механизм **квантирования**. Грубо говоря, "Вот тебе определённое количество времени, больше не дам, сколько бы процессов ни было". 
Такова общая модель. 

### 
Выделим, какие функции выполняет ОС для управления процессами. 
1. Создание процесса. 

Процесс -- структура данных ядра, т.е. создать процесс значит создать структуру данных. Есть принципиальное различие между созданием процессов в линуксе и винде. 

В линуксе они рождаются деревом планирования. Первый процесс `init` стартует ядро, 
Ядро запускает первый процесс `init`, все остальные процессы порождаются процессом `init`. Любой процесс хранит свой `pid` и `parent pid`. Зачем это нужно? Во-первых, в такой модели удобно обеспечить анализ всех дочерних процессов. Любой процесс, завершаясь, посылает родителю информацию о том, что он завершается, дальше этот сигнал можно как-то обработать. 
Естественный минус -- что произойдёт, если у меня работает дочерний процесс и у меня крашнулся родительский? Теперь дочерний процесс стал осиротевшим (да, это официальный термин). Были попытки починить это усыновлением (и это, как ни странно, тоже). Это требует большого количество ресурсов для обработки целых двух уровней процессов, а не одного. Поэтому сейчас работает другая модель: если у нас теряется связь с родителем, то всех таких потомком усыновляет `init`. Почему это так? Просто потому что `init` имеет обработчик всех сигналов, которых конечное число (кажется, 64, только два из них пользовательские). 
/* какие-то слова */
Зомби-процессом называется процесс, который завершился, родитель которого тоже завершился, но до него. Он не может получить информацию о том, что родительский процесс крашнулся. Он не может отослать родителю код возврата. Процесс уже не может жить и не может умереть. Чем они плохи? Они занимают дескриптор. Если аптайм каких-нибудь семь лет, зомби-процессы могут сожрать все pid'ы. 

В windows всё устроено по другому. Он использует "государственную модель". Есть диспетчер процессов, который управляет абсолютно всеми процессами. Когда процесс хочет создать дочерний, он делает системный вызов. Ядро вызывает сервис диспетчера процессов, он создаёт новый процесс и родителю сообщает просто идентификатор дочернего процесса родительскому. В линуксе создание процесса это два системных вызова: `fork()` и `exec()`.  Что это нам позволяет? На базовом уровне обеспечить серьёзную безопасность, потому что дочерний процесс не может получить больше данных, чем было у родителя. 
В винде дочерний процесс запрашивает те данные, которые ему нужны, и диспетчер процессов легко их ему выдаёт. Зато не бывает зомби-процессов.

2. Обеспечиваем процессы и потоки необходимыми ресурсами

Бывает первичное обеспечение процесса и последующее. 
Здесь тоже есть принципиальная разница: винда гарантирует каждому процессу чистое и независимое адресное пространство. Линукс клонирует родительское. Раньше это клонирование приводило к достаточно медленному созданию процесса. 

3. Изоляция процесса

Она происходит на нескольких уровнях. 
Во-первых, на уровне аппаратной поддержки ядра. Там реализована защита памяти: при каждом обращение к памяти происходит оценка, в чьё адресное пространство происходит обращение и срабатывает прерывание, если лезете не в свою память. 
В линуксе создана достаточно простая модель: существует каталог ```/var/lock```. Там находится символическая запись файла. Процесс создаётся, смотрит, есть ли файл в этом каталоге. Если есть -- обращение не позволено. 

4. Диспетчеризация потока

Смена потока на какой-нибудь другой. Она происходит в три этапа: сохранить регистровый контекст уходящего потока, загрузить новый регистровый контекст, сменить статус потока.
Процедура смены статуса должна быть атомарной. Здесь происходит коллизия с прерыванием. Поэтому большинство ОС использует переход в однопрограммный режим. Поэтому большинство процессоров поддерживают очень опасный режим игнорирования прерываний. Чем это плохо? Тем, что если я плохо написал код между этими двумя процессами, то я уже ничего не сделаю, потому что процессор игнорирует прерывание. 

6. Организация межпроцессного взаимодействия

Мы хотим, чтобы процессы были изолированны, но при этом могли обмениваться данными. Для этого нужен посредник. ОС гарантирует только одно: другой процесс получит сигнал, который перебросит ОС, а дальше он уже решает, обрабатывать его или нет. 
На примере линукса, существуют именованные каналы. Когда вы запускаете command1 | command2. Между ними существует буффер, чтобы обеспечить взаимодействие. На самом деле, этот буффер находит в пространстве процесса, который породил запуск этих подпроцессов. Что делать, если я нахожусь в изолированном процессе? Будет создан именованный канал. Будет создан "файл", в который одна программа пишет, а другая читает. Но если бы файл был физическим. это было бы очень долго. 

7. Синхронизация процессов и потоков

У меня есть два процесса, которые оба хотят использовать ресурс, который нельзя разделить. Как фиксить? ~вилкой~ Потребовалось десять лет, чтобы разработать теорию этого переключения. Дейкстра был одним из тех, кто смог решить эту проблему, но на тот момент его идея семафоров была нереализуема. 
Дальше, даже если я даже научился делать так, чтобы один неразделяемый ресурс давался одному процессу, проблемы не исчезли. Например, есть два процесса и два ресурса. Первый процесс заблочил первый ресурс, второй процесс заблочил второй ресурс. Теперь им потребовались ресурсы друг друга. Что это? Это deadlock. Они не могут завершиться, не могут продолжить выполнение. Об этом расскажут потом. 

8. Завершение и уничтожение потоков и процессов

Во-первых, мы понимаем, что в программировании ничего не идёт гладко. Во-вторых, все исключения никогда не обработаешь. ОС это такая конечная инстанция. Если всё пошло не по плану, что должна сделать ОС? Она должна обладать какими-то механизмами, которые будут завершать эти процессы (в том числе аварийно), и при этом сохранять данные. /* стори про windows 98 и ожидание программ */


## Пятая лекция
### Модель процессов в ОС
У нас есть ядро процесса, будем считать, что ядро только одно. Т.е. только один процесс может выполняться в один момент времени, при этом существует псевдомногопоточность. Любой процесс в один момент времени либо не исполняется, либо исполняется. 
``` mermaid
graph LR
A[Не исполняется] -- может перейти --> B[исполняется]
B -- может перейти --> A
```
Дальше создали трёхуровневую модель: 
``` mermaid
graph LR
A[Готовность]
A --> C[Исполняется]
C -- таймер --> A
C -- Ожидание чего-то --> B[Ожидание] 
B -- встаём в очередь--> A 
```
 Перед готовностью существует рождение:
``` mermaid 
 graph LR 
 A[Рождение] --> B[Готовность]
 B --> C[...]
 B --> D[...]
```

Состояние __рождения__ ввели, чтобы обеспечить фильтрацию: нужен буфер, который будет разрешать рождаться процессу только если существует не более n процессов,  иначе пусть он встаёт в очередь. 
Последняя схема, которая внедрена уже почти во все ОС:
``` mermaid
graph LR
Z[...] --> A
A[Рождение] --> B[Готовность]
B --> C[Исполняется]
C --> G[Исключение]
G --> B
C --> D[Завершение]
C --> B
C --> E
E[Ожидание] --> B
D --> F[...]
G --> D
```
Если мы часто попадаем одним процессом в исключения, то этот процесс будет завершён. Если же он будет исполнен до переполнения счётчика, счётчик будет обнулён и процесс продолжит крутиться в цикле. 

Можем начать рассуждать об __алгоритмах планирования__.
Итак, у меня в системе несколько сотен процессов и всего 16 ядер (их всегда мало) и мало памяти (её всегда мало). Плюс, у меня ограничена пропускная способность разных каналов. В общем, есть множество ограниченных ресурсов, суммарное желание потребителей больше, чем возможностей. В таком положении ОС находится всегда. 
 
**Горизонт планирования** -- на сколько шагов мы планируем задачи. 
Какова должна быть его глубина? Разделим задачи планирования по __разным__ горизонтам, обычно, на три. 

**Краткосрочное планирование** находится между готовностью и исполнением: это решение о том, какой процесс будет выполняться следующим. Здесь слишком часто происходят изменения, нужно быстро что-то выбирать, но и решения не очень судьбоносные для системы. Мы должны использовать на этом уровне какие-то простые алгоритмы планирования (желательно, чтобы они работали за константу). 

**Долгосрочное планирование** находится между рождением и готовностью. В рамках долгосрочного планирования я принимаю решение о рождении процесса. Оно сильно меняет состояние ОС, поэтому очень важно. 

**Среднесрочное планирование** (планирование подкачки): у меня процесс исполнялся, исполнялся и добрался до ввода-вывода. Допустим, ему нужен доступ к жёсткому диску, а в очереди он не скоро. На этом этапе мы можем его выкинуть из оперативной памяти, чтобы дать возможность родиться другому процессу, но очередь его мы придержим. Когда он в очереди подойдёт к началу, мы его вернём из памяти в оперативку. 
На высоконагруженных системах можно внедрить такую же очередь около готовности. 

Существует ещё планирование очередей ввода-вывода. Вообще, большинство современных интерфейсов работают последовательно, поэтому в ОС существует k очередей к каждому устройству ввода-выводу, каждой из которых управляет планировщик. 

[Фоточка-картиночка](https://lh3.googleusercontent.com/BTyVOWXgzLggg-sZNxmbV6udAjGYlnTfgfzIilXNV-2PmOhJwzJDHUXGmgMTfusbRjfM7WJQU6wa "Схема с доски")

Критерии планирования:
1. Критерий справедливости (морально обоснованный, но не работающий ~как коммунизм~)
2. Эффективность (хорошая утилизация ресурсов)
3. Сокращение времени ожидания
4. Сокращение полного времени выполнения
5. Сокращение времени отклика

Почти все эти критерии находятся в противоречии друг с другом. 

Ограничения:
1. Предсказуемость
2. Масштабируемость 
3. Минимальные расходы

### Параметры планирования
**Статические параметры системы** -- что-то долговременное, что почти не будет меняться. 
**Динамические параметры системы** -- в основном про наличие ресурсов. 

**Статические параметры процесса** -- некоторые характеристики процесса, которые не будут __почти__ меняться, например, какой пользователь породил его, приоритеты, какие библиотеки к нему подключены. 
**Динамические параметры процесса**: 
1. `CPU-birst` -- ожидаемое время непрерывной работы
2. `I/O-birst` -- если этот процесс уйдёт в IO, он пробудет там столько-то времени

## Шестая лекция
### Реализации моделей планирования
**Дисциплины обслуживания** делятся на вытесняющие и не вытесняющие. В случае вытесняющего у меня существует некоторый способ остановить выполнение текущего процесса, чтобы выполнить некоторый следующий.

Самая простая дисциплина обслуживания -- __FCFS__. 
Пусть у меня есть три процесса: Р1, Р2, Р3, я для этих процессов задам CPU-birst: 13, 4 и 1 соответственно. Тогда исполнение пойдёт следующим образом: 13 раз исполняется первый процесс, остальные в это время готовы. Второй процесс выполнился за четыре такта, третий процесс ждёт в готовности. Затем отрабатывает третий процесс.

Полное время вычисления = 18. 
Среднее время выполнения: `(13 + (13 + 4) + (13 + 4 + 1)) / 3` = 16
Среднее время ожидания: `(0 + 13 + 17) / 3` = 10

##
Теперь разверну очередь в другую сторону: Р3 -> P2 -> P1

Полное время выполнения = 18
Среднее время выполнения: `(18 + 5 + 1) / 3` = 8
Среднее время ожидания: `(5 + 1 + 0) / 3` = 2

Операционная система старается выполнить процессы так, чтобы они выполнялись как можно быстрее. Появилась следующая модель: давайте сделаем модель с возможностью вытеснения через кванты времени -- **Round Robin**. 
Представим, что я возьму некоторый квант, например, равный четырём. Теперь после каждых четырёх тактов выполнения я буду переключаться на выполнение следующего процесса. 

Полное время выполнения = 18
Среднее время выполнения: `(18 + 8 + 9) / 3` = 12
Среднее время ожидания:  `(5 + 4 + 8) / 3` = 5.6

Посмотрим другой вариант: возьмём другой размер кванта, например, единице. 

Полное время выполнения = 18
Среднее время выполнения: `(18 + 9 + 3) / 3` = 10
Среднее время ожидания: `(5 + 5 + 2) / 3` = 4

Существует взвешенный Round Robin: процессы не равноправны.

Видно, что логично было бы пропускать самые короткие процессы. Поэтому следующий механизм, который появился: **SJF** (shortest-job-first). 
Рассмотрим очередь из четырёх процессов P0, P1, P2 и Р3. CPU-birst 6, 2, 7 и 5 соответственно. Введём некоторое время, когда процессы появятся в очереди: 0, 2, 6 и 0 соответственно. Работаем по два такта: сначала выполняется последний, потом происходит переоценка приоритетов: появился второй процесс, которому нужно всего два такта на выполнение, он и будет выполнено. Через каждые два такта будет производиться переоценка и выбор следующего самого короткого процесса. 
Чем удобна эта схема: я ни в какой момент времени не пытаюсь вычислить порядок, я просто ищу самый короткий. Но даже этот подход не очень хорош, потому что в каждый момент времени я должен сравнить остаток тактов: при появлении JVM выяснилось, что управлять нужно сотнями и тысячами процессов, это порождало слишком много накладных расходов. 
Тогда начали думать, что предлагалось раньше, чтобы что-нибудь модернизировать. Один из таких вариантов: алгоритм **гарантированного планирования**.
Пусть у нас есть N пользователей, которые находятся в очереди в один момент времени. Тогда было бы справедливо, если бы каждый пользователь имел `1/N` машинного времени. Сделаем коэффициент справедливости: коэффициенты в формуле меняются в зависимости от времени, которое потратил человек. Если ты давно не отвечал, у тебя растёт знаменатель, если отвечаешь -- растёт числитель, уменьшая твой коэффициент перед остальными. 
Система сломалась мгновенно: открываешь сессию, долго ничего не грузишь, затем начинаешь посылать посылку за посылкой, т.к. коэффициент позволяет. 

Все эти алгоритмы имеют общую особенность: все коэффициенты определяются изнутри. Появляется идея внешнего определения коэффициентов. Как это работает? 
Многие ОС добавили в SJF понятие приоритета. Сначала решали, что процесс приоритетен. Если несколько процессов имеют одинаковый приоритет, работал SJF. Это клёво, но мы всё ещё зависим от N, хотелось бы от этого уйти. Решили идею SJF несколько развить и появилась идея многоуровневых очередей -- **multilevel queue**. Создаём много очередей с правилом: любой процесс может выполняться, только если нет никаких процессов в очереди с меньшим приоритетом. В каждой очереди работает SJF или RR, это неважно. Теперь у нас зависимость не от количества пользователей, а от количества очередей, что, конечно, является константой. 
Однако всё разбивается о реальность: в 1967 году в MIT запускается суперкомпьютер. Uptime машины был шесть лет. Экспериментально в системе была возможность задавать 255 приоритетов, некоторым образом ранжировали студентов. В 1973 году остановили машину и сняли полный дамп памяти, чтобы исследовать, что происходило. Обнаружили там несколько процессов, которые были поставлены в 1967 году и так никогда и не были выполнены. В MIT решили задачу следующим образом: для каждого процесса определяем некоторый предельный квант ожидания. Если мы превысили его значения в некоторой очереди, этому процессу на единицу повышают приоритет, т.е. переводят в более приоритетную очередь. Если он всё-таки выполнился, его забросят обратно туда, откуда он пришёл.
Система оказалось очень устойчивой, но были и проблемы: модель подразумевает, что SJF выполняется внутри одной очереди. Но я могу дать некоторому очень короткому процессу очень низкий приоритет, я бы мог его быстро выполнить. Переоценивать это постоянно сложно, поэтому решили сделать очередь с обратной связью:
я беру и каждой очереди назначаю  (по степеням двойки обычно) кванты выполнения. Любой процесс определяют в лучшую очередь, но выполняться непрерывно ему дают только, например, восемь тактов. Дальше несколько вариантов: он успел выполниться и ушёл сам, тогда, вернувшись, он опять попадёт в первую очередь. Иначе, его прервут на восьми тактах и отправят в соседнюю очередь. В ней ему дадут, когда никого не будет в прошлой очереди, уже 16 тактов. Таким образом, если процесс не хочет интерактива, он очень быстро попадёт в минимальный приоритет, но зато ему сразу давать будут много времени. 
Есть и в обратную сторону путь: если процесс несколько раз укладывается в меньшее время, его перебросят в соседнюю очередь с меньшим временем выполнения. Теперь все ОС поддерживают более-менее эту модель. 

Как это реализовано в концепции NT:
В windows всё делится на 32 очереди: с 0 по 15 и с 16 по 31. Чем выше номер очереди, тем круче. Они жёстко разделены на две части: сверху системные процессы, снизу user-space. Никакой пользовательский процесс не получит приоритет выше, чем самый неважный системный. Почему это сделано так? Это был единственный выход для MS из-за близости к микроядру, чтобы мы не могли заблокировать важные для системы сервисы своими процессами. Также компоненты стали стабильны: никто, кроме самого MS не изменит комбинацию системы. Поэтому на каждый момент времени я могу решить оптимизационную проблему: я её решил и теперь знаю, что на старте могу распределить процессы по очереди и это будет дальше работать нормально. 
Пользовательские процессы же могут двигаться внутри доступных им очередей. Кроме того, Windows сама двигает пользователей в соответствии с некоторыми признаками, например, если процесс пришёл и ушёл, ему дадут приоритет повыше, надеясь, что он снова придёт и уйдёт быстро. Кроме того, есть понятие "активного окна". Процесс, который находится в нём, получает некоторый плюс для своей работы. 

Как это ~у хохлов~ в Linux:
Тоже многоуровневые очереди, причём у нас 140 очередей, которые делятся на 100 и 40. Даже в linux и даже под root вы не можете управлять совсем уж очередями, вам выделено под управление 40 очередей, в них частично находятся и системные процессы. 100 остальных используются для буквально нескольких процессов. Они созданы для "реального времени". Дальше всё хитрее. Система может быть многопроцессорной, допустим, из двух процессоров. К каждому из них строятся две очереди по 140 очередей (всего четыре очереди). Некоторый процесс со своим значением приоритета попадает в очередь. Каждая очередь представляет либо активную очередь, либо инактивную. В конец активной очереди будет записан процесс. Если процесс завершился или прерван, он будет перекинуть в инактивную очередь, причём приоритет может измениться. Как только все элементы из активной очереди в этом месте кончатся, будет своп с инактивной частью очереди. Теперь учитывается и приоритет пользователя, и поведение процесса, и всё это не зависит от `N`. Официальное название этого планировщика `O(1)`. 

Это всё клёво, но до этого момента мы рассматривали независимые процессы, хотя это в реальности не так. Эти процессы могут быть порождены одним процессом, могут использовать общие ресурсы. 

## Седьмая лекция
### Взаимодействие процессов
Проблема в том, что процессы используют много общий структур данных, начиная с таблицы процессов. Надо примирить  противоречие: процессы должны взаимодействовать, но они ничего друг о друге не знают. 
В любом процессе, в его коде, существует **критическая секция управления ресурсами**. Относительно ОС процесс асинхронен, т.е. ОС Не может сказать, в какой момент времени начнётся вхождение процесса в критическую секцию ресурса. 
Логичное решение -- входя в секцию процесс должен сообщить ОС об этом, выходя -- тоже. Казалось бы, проблема решилась, но нет. Идеального варианта существовать не может, появилась совокупность проблем:
1. **Взаимоисключение**: мы должны обеспечить, чтобы два процесса не оказались одновременно в критической сессии относительно одного и того же ресурса. Тогда говорим, что если им нужен один и тот же ресурс, они попадают в race-condition. Отсюда другая проблема
2. Нам нельзя допустить **отсутствие прогресса**, т.е. когда ресурс свободен, есть процессы, которым он нужен, но процессы не могут им воспользоваться.  
3. **Отсутствие голодания**: идея приблизительно такая. У меня есть три процесса и некоторый ресурс. Пока процесс P0 получил этот ресурс, родился некоторый процесс P1, который через некоторое время получит этот ресурс. Через некоторое время родился процесс P2,  который ждёт ресурс, но перед тем как он его получит, снова рождается P0 и снова получает ресурс. Таким образом, P2 может потенциально бесконечно ждать
4. **Тупик (deadlock)**. У меня есть два процесса, у каждого из них есть критические секции относительно некоторого ресурса. Допустим, первому процессу сначала нужен только ресурс R0, потом одновременно R1 и R0. Другому процессу нужно ровно обратное. В такой ситуации оба процесса встают в deadlock.

Попытаемся разобраться с первыми двумя проблемами. 
**Переход в однопрограммный режим** -- в прологе я запрещаю прерывания, в эпилоге снова разрешаю. Проблема в том, что если код завис в режиме, где запрещены прерывания, будет прекращена работа всей вычислительной системы, поэтому так редко делают, например, при переключении процесса из режима running в sleep и наоборот. 
Как бороться:
* **Замок**.
Это не работает из-за устройства while()
```
shared int look = 0;
Pi() {
	...
	while (look) {
	look = 1;
	{
		critical section;
	}
	look = 0;
	...
}
``` 
Проблема в условии прогресса. Такой алгоритм реально используется в некоторых системах реального времени, где некоторые процессы будут постоянно конкурировать за критическую секцию и гарантированно обращаться к ней на короткое время.
```
shared int turn = 0;
Pi() {
	while (turn != i) {
		critical section 
	}
	turn = 1 - i; // ???
	...
}
```
Появилась мысль: наверное проблема в том, что мы пытаемся использовать одну переменную на все процессы.
Следующий подход с флагами готовности должен это решить. В нём не могут два процесса быть в критической секции одновременно. 
```
shared int ready[2] = {0, 0}
Pi() {
	...
	ready[i] = 1;
	while (ready[1 - i]) {
		critical section
	}
	ready[i] = 0;
	...
}
```
Проблема возникнет между строчками `ready[i] = 1` и `while()`. Два процесса могут одновременно поставить флаг и долго (потенциально бесконечно) ждать. 
В шестидесятых в итоге решили, что нельзя решить задачу алгоритмически, и считали так до 1981 года. 	
Петтерсон опубликовал свой алгоритм в 1981 году: представим, что у нас есть дверь, к которой подходят два человека одновременно. Подходит первый человек и говорит "После Вас". Ему отвечают "Нет, после Вас". После этого, по правилам этикета, первый проходит. 
Как это выглядит? 
```
shared int ready[2] = {0, 0}
shared int turn = 0;
Pi() {
	...
	ready[i] = 1;
	rutn = 1 - i;
	while (ready[1 - i] && turn == 1 - i) {
		critical section
	}
	ready[i] = 0;
	...
}
```
Когда дошло до программирования, выяснилось, что процессов могут быть тысячи. Передавать по кругу становится сложно, теперь тысячи стоят у двери и препираются. Когда подойдёт новый процесс, ругань пойдёт по новой. 
Единственная мысль, которая появилась у всех,  "было бы классно, если бы у меня появилась аппаратная конструкция, которая позволила бы выполнять в самом первом алгоритме атомарно переход после `while()`. Очень быстро появилась соответствующая конструкция. 
```
shared int lock = 0;
while (Test_and_Set (&look)) {
	critical section
}
lock = 0;
...
}
```
Проблема в том, что опрос переменной замка через эту инструкцию возможен только в режиме ядра. Это вызывает накладные расходы, но лучше ничего не сделать. 
Всё это позволило реализовать идею из шестидесятых, которая не могла быть решена из-за отсутствия аппаратной поддержки. Идею придумал Дейкстра, которую назвали "Семафор". 
Сегодня она реализована во всех ОС. 
Семафор -- это некоторая неотрицательная целочисленная переменная. Над ней разрешены две операции: 
1. P(s) -- проверять. 
```
while (s == 0);
s = s - 1;
``` 
2. V(s)
```
s = s + 1;
```
`while` должен быть атомарен. 

Проблема `Producer -- consumer`. 
```
Semaphor mutex = 1;
Semaphor empty = N;
Semaphor full = 0;

Producer() {
	while (1) {
		produce_data;
		P(empty);
		P(mutex);
		put_data;
		v(mutex);
		v(full);
	}	
}

Consumer() {
	while (1) {
		p(full);
		p(mutex);
		get_data;
		v(mutex);
		v(empty);
		consume_data;
	}
}
```

Вернёмся к голоданию и тупикам.
Проблему голодания можно решить с помощью управления приоритетами. С тупиками всё сложнее, после создания семафоров Дейкстра столкнулся именно с ними. Он придумал парадокс про [ "обедающих философов"](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE%D0%B1_%D0%BE%D0%B1%D0%B5%D0%B4%D0%B0%D1%8E%D1%89%D0%B8%D1%85_%D1%84%D0%B8%D0%BB%D0%BE%D1%81%D0%BE%D1%84%D0%B0%D1%85). Настоятельно рекомендую прочитать перед продолжением ботания. 
Как же этот "официант" должен узнавать, что существует тупиковая ситуация? 
Позже три математика из Беркли сформулировали формальные условия формирования тупика. Если все четыре условия выполнены, тупик будет. Если нет -- гарантированно не будет.
* Условие взаимоисключения
* Условие ожидания ресурсов
* Условие неперераспределяемости
* Круговое ожидание

Дальше пошли думать, что с этим делать?
1. Игнорировать
2. Предотвращать
3. Обнаруживать
4. Осуществлять восстановительные работы

В результате, большинство ОС решило пойти по самому простому пути -- игнорировать. 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTczOTk0MDQ3LC0yMDcxNjAyNTksMTIzNT
A0ODk0NiwtMzgzNjUyNjI2LDkwNDUzODYwOCwxNTk2MzY5MzYx
LC03Njc2MDk0OTQsLTEzNzYwNjQ2OTEsLTg3NDY0MDEwMCwtMj
YwOTAxOTYwLDUwNDE0MzI2LC0zNTAwMzM4MywzNjczNTE0ODMs
LTM5MDkzOTgwMywtMTgwNjY4NjY0MiwxNTg4ODcwNjI1LDEyNz
M0NjIwMzcsNDkyNDI2MjM0LDIxMDM2Njg2NTMsMTYyMTA5NTld
fQ==
-->